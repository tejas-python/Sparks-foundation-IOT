{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d46ac18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base path to YOLO directory\n",
    "MODEL_PATH = \"yolo-coco\"\n",
    "\n",
    "# initialize minimum probability to filter weak detections along with\n",
    "# the threshold when applying non-maxima suppression\n",
    "MIN_CONF = 0.3\n",
    "NMS_THRESH = 0.3\n",
    "\n",
    "# boolean indicating if NVIDIA CUDA GPU should be used\n",
    "USE_GPU = False\n",
    "\n",
    "# define the minimum safe distance (in pixels) that two people can be\n",
    "# from each other\n",
    "MIN_DISTANCE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf4e802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detectPeople(frame, net, ln, personIdx=0):\n",
    "\t# grab the dimensions of the frame and  initialize the list of\n",
    "\t# results\n",
    "\t(H, W) = frame.shape[:2]\n",
    "\tresults = []\n",
    "\n",
    "\t# construct a blob from the input frame and then perform a forward\n",
    "\t# pass of the YOLO object detector, giving us our bounding boxes\n",
    "\t# and associated probabilities\n",
    "\tblob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416),\n",
    "\t\tswapRB=True, crop=False)\n",
    "\tnet.setInput(blob)\n",
    "\tlayerOutputs = net.forward(ln)\n",
    "\n",
    "\t# initialize our lists of detected bounding boxes, centroids, and\n",
    "\t# confidences, respectively\n",
    "\tboxes = []\n",
    "\tcentroids = []\n",
    "\tconfidences = []\n",
    "\n",
    "\t# loop over each of the layer outputs\n",
    "\tfor output in layerOutputs:\n",
    "\t\t# loop over each of the detections\n",
    "\t\tfor detection in output:\n",
    "\t\t\t# extract the class ID and confidence (i.e., probability)\n",
    "\t\t\t# of the current object detection\n",
    "\t\t\tscores = detection[5:]\n",
    "\t\t\tclassID = np.argmax(scores)\n",
    "\t\t\tconfidence = scores[classID]\n",
    "\n",
    "\t\t\t# filter detections by (1) ensuring that the object\n",
    "\t\t\t# detected was a person and (2) that the minimum\n",
    "\t\t\t# confidence is met\n",
    "\t\t\tif classID == personIdx and confidence > MIN_CONF:\n",
    "\t\t\t\t# scale the bounding box coordinates back relative to\n",
    "\t\t\t\t# the size of the image, keeping in mind that YOLO\n",
    "\t\t\t\t# actually returns the center (x, y)-coordinates of\n",
    "\t\t\t\t# the bounding box followed by the boxes' width and\n",
    "\t\t\t\t# height\n",
    "\t\t\t\tbox = detection[0:4] * np.array([W, H, W, H])\n",
    "\t\t\t\t(centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "\t\t\t\t# use the center (x, y)-coordinates to derive the top\n",
    "\t\t\t\t# and and left corner of the bounding box\n",
    "\t\t\t\tx = int(centerX - (width / 2))\n",
    "\t\t\t\ty = int(centerY - (height / 2))\n",
    "\n",
    "\t\t\t\t# update our list of bounding box coordinates,\n",
    "\t\t\t\t# centroids, and confidences\n",
    "\t\t\t\tboxes.append([x, y, int(width), int(height)])\n",
    "\t\t\t\tcentroids.append((centerX, centerY))\n",
    "\t\t\t\tconfidences.append(float(confidence))\n",
    "\n",
    "\t# apply non-maxima suppression to suppress weak, overlapping\n",
    "\t# bounding boxes\n",
    "\tidxs = cv2.dnn.NMSBoxes(boxes, confidences, MIN_CONF, NMS_THRESH)\n",
    "\n",
    "\t# ensure at least one detection exists\n",
    "\tif len(idxs) > 0:\n",
    "\t\t# loop over the indexes we are keeping\n",
    "\t\tfor i in idxs.flatten():\n",
    "\t\t\t# extract the bounding box coordinates\n",
    "\t\t\t(x, y) = (boxes[i][0], boxes[i][1])\n",
    "\t\t\t(w, h) = (boxes[i][2], boxes[i][3])\n",
    "\n",
    "\t\t\t# update our results list to consist of the person\n",
    "\t\t\t# prediction probability, bounding box coordinates,\n",
    "\t\t\t# and the centroid\n",
    "\t\t\tr = (confidences[i], (x, y, x + w, y + h), centroids[i])\n",
    "\t\t\tresults.append(r)\n",
    "\n",
    "\t# return the list of results\n",
    "\treturn results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff884398",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from scipy.spatial import distance as dist\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import argparse\n",
    "import imutils\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b497974",
   "metadata": {},
   "outputs": [],
   "source": [
    "videoFile = r'pedestrians.mp4'\n",
    "configPath = r'yolo-coco/yolov3.cfg'\n",
    "weightsPath = r'yolo-coco/yolov3.weights'\n",
    "net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c506f554",
   "metadata": {},
   "outputs": [],
   "source": [
    "previewWhileProcess = False\n",
    "saveLocally = True\n",
    "writer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0434a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we are going to use ['yolo_82', 'yolo_94', 'yolo_106']\n"
     ]
    }
   ],
   "source": [
    "ln = net.getLayerNames()\n",
    "ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "print(\"we are going to use\", ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86c6acaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 531\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cam = cv2.VideoCapture(videoFile)\n",
    "fps = int(cam.get(cv2.CAP_PROP_FPS))\n",
    "totalFPS = int(cam.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "computedFps = 0\n",
    "print(fps, totalFPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cda0a80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of objects Model hold:  80\n",
      "We gonna use person Only index of:  0\n"
     ]
    }
   ],
   "source": [
    "LABELS = []\n",
    "with open(\"yolo-coco/coco.names\", 'rt') as f:\n",
    "    LABELS = f.read().rstrip(\"\\n\").split('\\n')\n",
    "print(\"Number of objects Model hold: \", len(LABELS))\n",
    "print(\"We gonna use person Only index of: \",LABELS.index(\"person\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "028e63d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_violation = (0,0, 255)\n",
    "color_safe = (0,255,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a293a7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] init Process..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb680bf10b142468381661a42226d6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EOF Process..\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] init Process..\")\n",
    "pbar = tqdm(total=totalFPS, desc=\"Progress\") #progressBar\n",
    "while True:\n",
    "    success, frame = cam.read()\n",
    "    \n",
    "    if not success:\n",
    "        print(\"[INFO] EOF Process..\")\n",
    "        break        \n",
    "    \n",
    "    frame = imutils.resize(frame, width=700)\n",
    "    results = detectPeople(frame, net, ln, personIdx=LABELS.index('person'))\n",
    "\n",
    "\n",
    "    voilation = set()\n",
    "\n",
    "    if len(results) >=2:\n",
    "        centroids = np.array([r[2] for r in results])\n",
    "        distanceMap = dist.cdist(centroids, centroids, metric = \"euclidean\")\n",
    "\n",
    "#   print(distanceMap.shape)\n",
    "        for i in range(distanceMap.shape[0]):\n",
    "            for j in range(i+1, distanceMap.shape[1]):\n",
    "\n",
    "                #if distance between 2person is less minDistance\n",
    "                if distanceMap[i, j] < MIN_DISTANCE:\n",
    "                    voilation.add(i)\n",
    "                    voilation.add(j)\n",
    "\n",
    "\n",
    "        #Loop over results and make visualizations\n",
    "    for (i, (prob, bbox, centroid)) in enumerate(results):\n",
    "        (startX, startY, endX, endY)  = bbox\n",
    "        (cX, cY) = centroid\n",
    "\n",
    "        color = color_safe\n",
    "\n",
    "        if i in voilation:\n",
    "            color = color_violation\n",
    "\n",
    "        cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2) #bounding box\n",
    "        cv2.circle(frame, (cX, cY), 5, color, 1)\n",
    "    text = \"Social Distancing Violations: {}\".format(len(voilation))\n",
    "    cv2.putText(frame, text, (10, frame.shape[0] - 25),cv2.FONT_HERSHEY_SIMPLEX, 0.85, (0, 0, 255), 3)\n",
    "\n",
    "    #should be display\n",
    "    if previewWhileProcess:\n",
    "        cv2.imshow(\"Social Distance\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "    \n",
    "    #init save file\n",
    "    if saveLocally and writer is None:\n",
    "#         print(size)\n",
    "        fourcc =cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "        writer = cv2.VideoWriter('output.avi', fourcc, 20, (frame.shape[1], frame.shape[0]), True) \n",
    "    \n",
    "    if writer is not None:\n",
    "        writer.write(frame)\n",
    "        \n",
    "    #ProgressBar \n",
    "    computedFps +=1\n",
    "    pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "writer.release()     \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c054570",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
